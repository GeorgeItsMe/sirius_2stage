{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error\n",
    "import tqdm\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtrain = pd.read_csv('upd_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dtrain\n",
    "\n",
    "# Преобразование 'no' в 0, 'yes' в 1 для указанных столбцов\n",
    "columns_to_convert = ['thermal_power_plant_raion', 'incineration_raion', 'oil_chemistry_raion',\n",
    "                      'radiation_raion', 'railroad_terminal_raion', 'big_market_raion',\n",
    "                      'nuclear_reactor_raion', 'detention_facility_raion', 'culture_objects_top_25',\n",
    "                      'water_1line', 'big_road1_1line', 'railroad_1line']\n",
    "\n",
    "for column in columns_to_convert:\n",
    "    data[column] = data[column].apply(lambda x: 1 if x == 'yes' else 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>full_sq</th>\n",
       "      <th>life_sq</th>\n",
       "      <th>floor</th>\n",
       "      <th>max_floor</th>\n",
       "      <th>material</th>\n",
       "      <th>build_year</th>\n",
       "      <th>num_room</th>\n",
       "      <th>kitch_sq</th>\n",
       "      <th>...</th>\n",
       "      <th>cafe_count_5000_price_high</th>\n",
       "      <th>big_church_count_5000</th>\n",
       "      <th>church_count_5000</th>\n",
       "      <th>mosque_count_5000</th>\n",
       "      <th>leisure_count_5000</th>\n",
       "      <th>sport_count_5000</th>\n",
       "      <th>market_count_5000</th>\n",
       "      <th>price_doc</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8059</td>\n",
       "      <td>2013-05-21</td>\n",
       "      <td>11</td>\n",
       "      <td>11.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1907</td>\n",
       "      <td>1.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>...</td>\n",
       "      <td>26</td>\n",
       "      <td>133</td>\n",
       "      <td>207</td>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>161</td>\n",
       "      <td>10</td>\n",
       "      <td>2750000</td>\n",
       "      <td>5</td>\n",
       "      <td>1907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8138</td>\n",
       "      <td>2013-05-25</td>\n",
       "      <td>53</td>\n",
       "      <td>30.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1980</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>46</td>\n",
       "      <td>5</td>\n",
       "      <td>9000000</td>\n",
       "      <td>5</td>\n",
       "      <td>1980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8156</td>\n",
       "      <td>2013-05-27</td>\n",
       "      <td>77</td>\n",
       "      <td>41.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2014</td>\n",
       "      <td>3.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>7011550</td>\n",
       "      <td>5</td>\n",
       "      <td>2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8157</td>\n",
       "      <td>2013-05-27</td>\n",
       "      <td>45</td>\n",
       "      <td>27.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1970</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>3</td>\n",
       "      <td>7100000</td>\n",
       "      <td>5</td>\n",
       "      <td>1970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8178</td>\n",
       "      <td>2013-05-28</td>\n",
       "      <td>38</td>\n",
       "      <td>20.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1982</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>95</td>\n",
       "      <td>4</td>\n",
       "      <td>6450000</td>\n",
       "      <td>5</td>\n",
       "      <td>1982</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 294 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     id   timestamp  full_sq  life_sq  floor  max_floor  material  build_year  \\\n",
       "0  8059  2013-05-21       11     11.0    2.0        5.0       2.0        1907   \n",
       "1  8138  2013-05-25       53     30.0   10.0       16.0       1.0        1980   \n",
       "2  8156  2013-05-27       77     41.0    2.0       17.0       6.0        2014   \n",
       "3  8157  2013-05-27       45     27.0    6.0        9.0       1.0        1970   \n",
       "4  8178  2013-05-28       38     20.0   15.0       16.0       1.0        1982   \n",
       "\n",
       "   num_room  kitch_sq  ...  cafe_count_5000_price_high big_church_count_5000  \\\n",
       "0       1.0      12.0  ...                          26                   133   \n",
       "1       2.0       8.0  ...                           0                    11   \n",
       "2       3.0      12.0  ...                           0                     1   \n",
       "3       2.0       6.0  ...                           0                     3   \n",
       "4       1.0       8.0  ...                           1                    11   \n",
       "\n",
       "  church_count_5000  mosque_count_5000  leisure_count_5000  sport_count_5000  \\\n",
       "0               207                  1                  89               161   \n",
       "1                21                  1                   0                46   \n",
       "2                 7                  1                   0                12   \n",
       "3                 8                  1                   0                19   \n",
       "4                25                  1                   7                95   \n",
       "\n",
       "   market_count_5000  price_doc  month  year  \n",
       "0                 10    2750000      5  1907  \n",
       "1                  5    9000000      5  1980  \n",
       "2                  1    7011550      5  2014  \n",
       "3                  3    7100000      5  1970  \n",
       "4                  4    6450000      5  1982  \n",
       "\n",
       "[5 rows x 294 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1907\n",
       "1    1980\n",
       "2    2014\n",
       "3    1970\n",
       "4    1982\n",
       "Name: build_year, dtype: int64"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['build_year'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество NaN в столбце build_year: 0\n",
      "Количество записей в столбце: 16866\n"
     ]
    }
   ],
   "source": [
    "# Предположим, что ваш датасет называется data\n",
    "nan_count = data['build_year'].isna().sum()\n",
    "print(\"Количество NaN в столбце build_year:\", nan_count)\n",
    "print(\"Количество записей в столбце:\", len(data['build_year']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data = data.dropna(subset=['build_year'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['build_year'] = data['build_year'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        1907\n",
       "1        1980\n",
       "2        2014\n",
       "3        1970\n",
       "4        1982\n",
       "         ... \n",
       "16861    2017\n",
       "16862    1975\n",
       "16863    1935\n",
       "16864    2003\n",
       "16865    1968\n",
       "Name: build_year, Length: 16866, dtype: int32"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['build_year']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = data['price_doc']\n",
    "data['year'] = data['build_year']\n",
    "important_cols = ['material', 'num_room', 'sub_area', 'year', 'full_sq', 'max_floor', 'build_year', 'school_km', 'kremlin_km', 'floor', 'cafe_avg_price_5000']\n",
    "num_cat = ['material', 'num_room']\n",
    "cat = num_cat + ['sub_area']\n",
    "df = data[important_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('upd_train.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['id', 'timestamp', 'full_sq', 'life_sq', 'floor', 'max_floor',\n",
      "       'material', 'build_year', 'num_room', 'kitch_sq',\n",
      "       ...\n",
      "       'cafe_count_5000_price_high', 'big_church_count_5000',\n",
      "       'church_count_5000', 'mosque_count_5000', 'leisure_count_5000',\n",
      "       'sport_count_5000', 'market_count_5000', 'price_doc', 'month', 'year'],\n",
      "      dtype='object', length=294)\n"
     ]
    }
   ],
   "source": [
    "print(data.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Класс для создания кастомного датасета\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, dataframe, numeric_cols, categorical_cols, target_col):\n",
    "        self.numeric_cols = numeric_cols\n",
    "        self.categorical_cols = categorical_cols\n",
    "        self.target_col = target_col\n",
    "        \n",
    "        # Используем SimpleImputer для заполнения пропущенных значений в числовых столбцах\n",
    "        self.numeric_imputer = SimpleImputer(strategy='mean')\n",
    "        dataframe.loc[:, self.numeric_cols] = self.numeric_imputer.fit_transform(dataframe.loc[:, self.numeric_cols])\n",
    "        \n",
    "        # Используем LabelEncoder для кодирования категориальных столбцов\n",
    "        self.label_encoders = {}\n",
    "        for col in self.categorical_cols:\n",
    "            label_encoder = LabelEncoder()\n",
    "            dataframe.loc[:, col] = label_encoder.fit_transform(dataframe.loc[:, col])\n",
    "            self.label_encoders[col] = label_encoder\n",
    "        \n",
    "        # Масштабирование числовых признаков\n",
    "        self.scaler = StandardScaler()\n",
    "        dataframe.loc[:, self.numeric_cols] = self.scaler.fit_transform(dataframe.loc[:, self.numeric_cols])\n",
    "        \n",
    "        # Разделение данных на признаки и целевую переменную\n",
    "        self.X = dataframe.drop(columns=[self.target_col])\n",
    "        self.y = dataframe[self.target_col]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Получение данных для элемента с индексом idx\n",
    "        features = self.X.iloc[idx].values.astype('float32')\n",
    "        target = self.y.iloc[idx]\n",
    "        return torch.tensor(features), torch.tensor(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Maksim\\AppData\\Local\\Temp\\ipykernel_33412\\1196066216.py:21: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[0.06190765 0.22490958 0.30082828 ... 0.12442894 0.27626635 0.19811474]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  dataframe.loc[:, self.numeric_cols] = self.scaler.fit_transform(dataframe.loc[:, self.numeric_cols])\n",
      "C:\\Users\\Maksim\\AppData\\Local\\Temp\\ipykernel_33412\\1196066216.py:21: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[-1.98289483 -0.02278401  1.09727932 ...  1.51730306  0.49057835\n",
      " -0.48947706]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  dataframe.loc[:, self.numeric_cols] = self.scaler.fit_transform(dataframe.loc[:, self.numeric_cols])\n",
      "C:\\Users\\Maksim\\AppData\\Local\\Temp\\ipykernel_33412\\1196066216.py:21: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[0.06190765 0.22490958 0.30082828 ... 0.12442894 0.27626635 0.19811474]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  dataframe.loc[:, self.numeric_cols] = self.scaler.fit_transform(dataframe.loc[:, self.numeric_cols])\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('upd_train.csv')\n",
    "\n",
    "# Выбор важных столбцов\n",
    "important_cols = ['material', 'num_room', 'sub_area', 'year', 'full_sq', 'max_floor', \n",
    "                  'build_year', 'school_km', 'kremlin_km', 'floor', 'cafe_avg_price_5000', 'price_doc']\n",
    "numeric_cols = ['num_room', 'year', 'full_sq', 'max_floor', 'build_year', 'school_km', 'kremlin_km', 'floor', 'cafe_avg_price_5000']\n",
    "cat_cols = ['material', 'sub_area']\n",
    "\n",
    "\n",
    "\n",
    "# Создание кастомного датасета\n",
    "custom_dataset = CustomDataset(dataframe=data[important_cols], \n",
    "                               numeric_cols=numeric_cols, \n",
    "                               categorical_cols=cat_cols, \n",
    "                               target_col='price_doc')\n",
    "\n",
    "# Создание даталоадера\n",
    "batch_size = 32\n",
    "shuffle = True\n",
    "data_loader = DataLoader(dataset=custom_dataset, batch_size=batch_size, shuffle=shuffle)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(dataset, train_ratio=0.8, val_ratio=0.1, test_ratio=0.1, shuffle=True):\n",
    "    dataset_size = len(dataset)\n",
    "    indices = list(range(dataset_size))\n",
    "    if shuffle:\n",
    "        np.random.shuffle(indices)\n",
    "        \n",
    "    train_split = int(np.floor(train_ratio * dataset_size))\n",
    "    val_split = int(np.floor(val_ratio * dataset_size)) + train_split\n",
    "    \n",
    "    train_indices = indices[:train_split]\n",
    "    val_indices = indices[train_split:val_split]\n",
    "    test_indices = indices[val_split:]\n",
    "    \n",
    "    train_dataset = Subset(dataset, train_indices)\n",
    "    val_dataset = Subset(dataset, val_indices)\n",
    "    test_dataset = Subset(dataset, test_indices)\n",
    "    \n",
    "    return train_dataset, val_dataset, test_dataset\n",
    "\n",
    "# Разделение датасета на тренировочную, валидационную и тестовую выборки\n",
    "train_dataset, val_dataset, test_dataset = split_dataset(custom_dataset)\n",
    "\n",
    "# Создание DataLoader для каждого датасета\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция для одной эпохи обучения\n",
    "def train_epoch(model, train_loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    total_mae = 0.0\n",
    "    total_mape = 0.0\n",
    "    \n",
    "    for inputs, targets in tqdm(train_loader, desc=\"Training\"):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        loss = criterion(outputs.squeeze(), targets.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        total_mae += mean_absolute_error(targets.cpu().numpy(), outputs.detach().cpu().numpy())\n",
    "        total_mape += mean_absolute_percentage_error(targets.cpu().numpy(), outputs.detach().cpu().numpy())\n",
    "    \n",
    "    mean_loss = total_loss / len(train_loader)\n",
    "    mean_mae = total_mae / len(train_loader)\n",
    "    mean_mape = total_mape / len(train_loader)\n",
    "    \n",
    "    return mean_loss, mean_mae, mean_mape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция для оценки модели на валидационном датасете\n",
    "def evaluate_model(model, val_loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_mae = 0.0\n",
    "    total_mape = 0.0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in tqdm(val_loader, desc=\"Validation\"):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            loss = criterion(outputs.squeeze(), targets.float())\n",
    "            total_loss += loss.item()\n",
    "            total_mae += mean_absolute_error(targets.cpu().numpy(), outputs.cpu().numpy())\n",
    "            total_mape += mean_absolute_percentage_error(targets.cpu().numpy(), outputs.cpu().numpy())\n",
    "    \n",
    "    mean_loss = total_loss / len(val_loader)\n",
    "    mean_mae = total_mae / len(val_loader)\n",
    "    mean_mape = total_mape / len(val_loader)\n",
    "    \n",
    "    return mean_loss, mean_mae, mean_mape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomSequentialModel(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(CustomSequentialModel, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, 512),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "    def predict(self, numeric_data, categorical_data, cat_encoders, scaler):\n",
    "        numeric_data = scaler.transform(numeric_data)\n",
    "        categorical_data = [encoder.transform([val]) for encoder, val in zip(cat_encoders, categorical_data)]\n",
    "        numeric_tensor = torch.tensor(numeric_data, dtype=torch.float32).unsqueeze(0)\n",
    "        categorical_tensor = torch.tensor(categorical_data, dtype=torch.int64)\n",
    "        with torch.no_grad():\n",
    "            output = self(numeric_tensor, categorical_tensor)\n",
    "        return output.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Инициализация модели и отправка на GPU\n",
    "input_dim = len(numeric_cols) + len(cat_cols)\n",
    "model = CustomSequentialModel(input_dim).to('cuda')\n",
    "\n",
    "# Определение функции потерь и оптимизатора\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(dataset, train_ratio=0.8, val_ratio=0.1, test_ratio=0.1, shuffle=True):\n",
    "    dataset_size = len(dataset)\n",
    "    indices = list(range(dataset_size))\n",
    "    if shuffle:\n",
    "        np.random.shuffle(indices)\n",
    "        \n",
    "    train_split = int(np.floor(train_ratio * dataset_size))\n",
    "    val_split = int(np.floor(val_ratio * dataset_size)) + train_split\n",
    "    \n",
    "    train_indices = indices[:train_split]\n",
    "    val_indices = indices[train_split:val_split]\n",
    "    test_indices = indices[val_split:]\n",
    "    \n",
    "    train_dataset = Subset(dataset, train_indices)\n",
    "    val_dataset = Subset(dataset, val_indices)\n",
    "    test_dataset = Subset(dataset, test_indices)\n",
    "    \n",
    "    return train_dataset, val_dataset, test_dataset\n",
    "\n",
    "# Разделение датасета на тренировочную, валидационную и тестовую выборки\n",
    "train_dataset, val_dataset, test_dataset = split_dataset(custom_dataset)\n",
    "\n",
    "# Создание DataLoader для каждого датасета\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, test_loader, optimizer, criterion, device, num_epochs):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()  # set model to training mode\n",
    "        train_loss, train_mae, train_mape = 0.0, 0.0, 0.0\n",
    "        \n",
    "        progress_bar = tqdm(train_loader, desc=f'Epoch {epoch + 1}/{num_epochs}', leave=False)\n",
    "        for inputs, targets in progress_bar:\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()  # zero the parameter gradients\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs.squeeze(), targets.float())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item() * inputs.size(0)\n",
    "            abs_errors = torch.abs(outputs.squeeze() - targets.float())\n",
    "            train_mae += torch.sum(abs_errors).item()\n",
    "            train_mape += torch.sum(abs_errors / targets.float()).item()\n",
    "            \n",
    "            progress_bar.set_postfix({'loss': train_loss / len(train_loader.dataset), \n",
    "                                      'MAE': train_mae / len(train_loader.dataset),\n",
    "                                      'MAPE': train_mape / len(train_loader.dataset)})\n",
    "        \n",
    "        # Evaluation on validation dataset\n",
    "        val_loss, val_mae, val_mape = evaluate_model(model, val_loader, criterion, device)\n",
    "        \n",
    "        # Evaluation on test dataset\n",
    "        test_loss, test_mae, test_mape = evaluate_model(model, test_loader, criterion, device)\n",
    "        \n",
    "        print(f\"Train Loss: {train_loss / len(train_loader.dataset):.4f} | Train MAE: {train_mae / len(train_loader.dataset):.4f} | Train MAPE: {train_mape / len(train_loader.dataset):.4f}\")\n",
    "        print(f\"Val Loss: {val_loss:.4f} | Val MAE: {val_mae:.4f} | Val MAPE: {val_mape:.4f}\")\n",
    "        print(f\"Test Loss: {test_loss:.4f} | Test MAE: {test_mae:.4f} | Test MAPE: {test_mape:.4f}\")\n",
    "\n",
    "def evaluate_model(model, dataloader, criterion, device):\n",
    "    model.eval()  # set model to evaluation mode\n",
    "    total_loss, total_mae, total_mape = 0.0, 0.0, 0.0\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in dataloader:\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs.squeeze(), targets.float())\n",
    "            total_loss += loss.item() * inputs.size(0)\n",
    "            abs_errors = torch.abs(outputs.squeeze() - targets.float())\n",
    "            total_mae += torch.sum(abs_errors).item()\n",
    "            total_mape += torch.sum(abs_errors / targets.float()).item()\n",
    "    \n",
    "    avg_loss = total_loss / len(dataloader.dataset)\n",
    "    avg_mae = total_mae / len(dataloader.dataset)\n",
    "    avg_mape = total_mape / len(dataloader.dataset)\n",
    "    return avg_loss, avg_mae, avg_mape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 55722201931895.0078 | Train MAE: 5626682.3398 | Train MAPE: 0.8204\n",
      "Val Loss: 36499227005865.7578 | Val MAE: 4268333.0202 | Val MAPE: 0.7255\n",
      "Test Loss: 39236911674877.5703 | Test MAE: 4331097.2464 | Test MAPE: 0.7366\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 35854846074906.7188 | Train MAE: 4254212.0795 | Train MAPE: 0.7541\n",
      "Val Loss: 30336418086173.4570 | Val MAE: 3863601.7556 | Val MAPE: 0.6730\n",
      "Test Loss: 32886788990675.1094 | Test MAE: 3931919.1659 | Test MAPE: 0.6950\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 26560096185951.4766 | Train MAE: 3638976.1714 | Train MAPE: 0.6965\n",
      "Val Loss: 19124167723000.7109 | Val MAE: 3112226.6050 | Val MAPE: 0.6428\n",
      "Test Loss: 22061347496673.6680 | Test MAE: 3270148.2844 | Test MAPE: 0.7092\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 18332288926342.0352 | Train MAE: 2979199.9265 | Train MAPE: 0.6424\n",
      "Val Loss: 13959035653041.0430 | Val MAE: 2625643.5421 | Val MAPE: 0.5721\n",
      "Test Loss: 17168890543584.4551 | Test MAE: 2793040.4242 | Test MAPE: 0.6519\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 16188005964097.1953 | Train MAE: 2742952.7314 | Train MAPE: 0.6192\n",
      "Val Loss: 12849647242880.1523 | Val MAE: 2491917.0463 | Val MAPE: 0.5428\n",
      "Test Loss: 15960558553291.8301 | Test MAE: 2639212.9763 | Test MAPE: 0.6183\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 15552496644485.8066 | Train MAE: 2627674.2289 | Train MAPE: 0.6162\n",
      "Val Loss: 12364050531781.0859 | Val MAE: 2389875.7319 | Val MAPE: 0.5319\n",
      "Test Loss: 15455217889260.5879 | Test MAE: 2542018.4123 | Test MAPE: 0.6077\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 15086228574046.4922 | Train MAE: 2536996.3673 | Train MAPE: 0.6014\n",
      "Val Loss: 11772485977716.0039 | Val MAE: 2297575.8102 | Val MAPE: 0.5257\n",
      "Test Loss: 14933015119299.3359 | Test MAE: 2455753.8341 | Test MAPE: 0.6053\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 14561121673806.1738 | Train MAE: 2467904.6223 | Train MAPE: 0.5957\n",
      "Val Loss: 11380862259651.8711 | Val MAE: 2216652.0712 | Val MAPE: 0.5238\n",
      "Test Loss: 14639661253539.7910 | Test MAE: 2384072.5474 | Test MAPE: 0.6073\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 14423451133480.2246 | Train MAE: 2411921.1017 | Train MAPE: 0.5883\n",
      "Val Loss: 11195322577629.6836 | Val MAE: 2178022.6833 | Val MAPE: 0.5135\n",
      "Test Loss: 14363722406344.1895 | Test MAE: 2341432.0616 | Test MAPE: 0.5941\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 13925756957115.2383 | Train MAE: 2366258.8802 | Train MAPE: 0.5856\n",
      "Val Loss: 10962242359145.3770 | Val MAE: 2112153.2361 | Val MAPE: 0.5195\n",
      "Test Loss: 14239975460106.9199 | Test MAE: 2288943.7156 | Test MAPE: 0.6034\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 13638247005872.8398 | Train MAE: 2335563.1204 | Train MAPE: 0.5809\n",
      "Val Loss: 11321596359778.3906 | Val MAE: 2236742.8233 | Val MAPE: 0.4937\n",
      "Test Loss: 14265360173104.5312 | Test MAE: 2380473.0687 | Test MAPE: 0.5668\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 13525028642803.8574 | Train MAE: 2297836.2238 | Train MAPE: 0.5731\n",
      "Val Loss: 10807887860980.1562 | Val MAE: 2117557.3238 | Val MAPE: 0.4949\n",
      "Test Loss: 13873966036632.8711 | Test MAE: 2276626.3744 | Test MAPE: 0.5725\n"
     ]
    }
   ],
   "source": [
    "# Параметры обучения\n",
    "num_epochs = 12\n",
    "\n",
    "\n",
    "# Обучение модели\n",
    "train_model(model, train_loader, val_loader, test_loader, optimizer, criterion, 'cuda', num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "important_cols = ['material', 'num_room', 'sub_area', 'year', 'full_sq', 'max_floor', \n",
    "                  'build_year', 'school_km', 'kremlin_km', 'floor', 'cafe_avg_price_5000', 'price_doc']\n",
    "numeric_cols = ['num_room', 'year', 'full_sq', 'max_floor', 'build_year', 'school_km', 'kremlin_km', 'floor', 'cafe_avg_price_5000']\n",
    "cat_cols = ['material', 'sub_area']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted price: 5335407.5\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('model.pth'))\n",
    "\n",
    "# Выбор важных столбцов\n",
    "important_cols = ['material', 'num_room', 'sub_area', 'year', 'full_sq', 'max_floor', \n",
    "                  'build_year', 'school_km', 'kremlin_km', 'floor', 'cafe_avg_price_5000', 'price_doc']\n",
    "numeric_cols = ['num_room', 'year', 'full_sq', 'max_floor', 'build_year', 'school_km', 'kremlin_km', 'floor', 'cafe_avg_price_5000']\n",
    "cat_cols = ['material', 'sub_area']\n",
    "\n",
    "# Создание экземпляра кастомного датасета для первой строки\n",
    "first_row_data = data[important_cols].iloc[[0]]\n",
    "\n",
    "# Создание экземпляра кастомного датасета для первой строки\n",
    "first_row_dataset = CustomDataset(first_row_data, numeric_cols, cat_cols, target_col='price_doc')\n",
    "\n",
    "# Создание DataLoader для этого датасета (batch_size=1, так как у нас только одно наблюдение)\n",
    "first_row_loader = DataLoader(first_row_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "# Перевод модели в режим оценки (evaluation mode)\n",
    "model.eval()\n",
    "\n",
    "# Передача данных через модель и получение предсказанного значения\n",
    "with torch.no_grad():\n",
    "    for inputs, _ in first_row_loader:\n",
    "        inputs = inputs.to('cuda')  # отправляем данные на устройство (GPU)\n",
    "        output = model(inputs)  # получаем выход модели\n",
    "\n",
    "# Получение предсказанного значения\n",
    "predicted_price = output.item()\n",
    "print(\"Predicted price:\", predicted_price)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Реальная цена недвижимости: 5850000 рублей\n",
      "Предсказанная цена: 5335408 рублей\n"
     ]
    }
   ],
   "source": [
    "dtrain = pd.read_csv('train.csv')\n",
    "real_price = dtrain['price_doc'][0]\n",
    "print(f'Реальная цена недвижимости: {real_price} рублей')\n",
    "print(f'Предсказанная цена: {round(predicted_price)} рублей')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Разница в предсказании - 514592 рублей\n"
     ]
    }
   ],
   "source": [
    "print(f'Разница в предсказании - {real_price - round(predicted_price)} рублей')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
